\documentclass[a4paper]{article}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{gensymb}
\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\lstset{language=C,
numberstyle=\footnotesize,
basicstyle=\ttfamily\footnotesize,
numbers=left,
stepnumber=1,
frame=shadowbox,
breaklines=true}
\setcounter{secnumdepth}{0}


\begin{document}
\SweaveOpts{concordance=TRUE}

\title{Product Line Record Linkage in Consumer Product Data using Approximate String Matching and Clustering Methods}
\author{Riki Saito}

\maketitle
\thispagestyle{empty}
\newpage

\tableofcontents
\newpage


\section{Introduction}

Record linkage is a data cleaning task of identifying records that belong to the same entity in a single data set or across multiple data sets. It is necessary in aggregating or joining data by some entity that may contain duplicates but do not share a common unique identifier due to slight differences or inconsistencies. Compared to a "brute force" approach of linking records by hand, record linkage is an elegant solution to combining duplicate, redundant, or even similar (but not quite the same) records. It has applications in a wide variety of fields; for instance, in health data linking medical records of the same individuals may be necessary.  

In this paper we focus on the application of record linkage in consumer product data. The online retail market has grown rapidly fast with the rise of the digital age, and with that the availability of data has also increased substantially. 


\section{Background}

Consumer product data, particularly sales data, is an absolute necessity for any business to do business and market analytics. Sales data are generally available at the product level and specific date/time, and analysis is conducted in various ways. Analytics are used to evaluate sales performance of products aggegated by some time period (month, season, year, etc), estimate the percentage of shares the products dominate the market, compare the performance of one's products against competitor products in the same market, and more. These analyses are essential in business strategy and decision making. 

In current times sales data are widely available and can be obtained easily. Many businesses often use third party for-purchase data sources from  market research companies such as NPD or GFK. However, a large issue in joining or aggregating data on consumer products is that, compared to data like medical records of individuals, there are a lot more ways of representing duplicate records, and the data structure/hierarchy may not be clearly outlined. Let's first define some key terms:


\subsection{Definitions}

\textbf{Product Line (PL)}:  a group of products comprising of different sizes, colors, or types, produced or sold under one unique product or model name (i.e. Apple iPhone 5, UE Boom 2)

\textbf{Stocking Keeping Unit (SKU)}: a particular product identified by its product line as well as by its size, color, or type, where its identification is typically used for inventory purposes (i.e. Apple iPhone 5 64GB Black, UE Boom 2 Blue)

\textbf{Distance}: an estimated numerical representation of how far apart two objects are. Can be interchangeable with \textbf{Dissimilarity}

\textbf{Group}: The actual class of one or more record(s) that belong to the same entity

\textbf{Cluster}: The predicted class of one or more record(s) that belong to the same entity

\textbf{Block}: 


\subsection{Record Linkage in Consumer Product Data}

With the increasing size and availability of data, the need for maintaining data quality is an issue that has surged in sales analytics.

A typical data hierarchy in market data might appear like this:

\begin{table}[]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{|l|l|l|}
\hline
Data Hierarchy     & Description                                                         & Example                                       \\ \hline
Category           & Category, or type of product                                        & Pointing Devices                              \\ \hline
Brand              & Name of Brand or Company                                            & Logitech                                      \\ \hline
Product Line       & Name of Product Model                                               & M325 Wireless Optical Scroll Mouse            \\ \hline
Stock Keeping Unit & Product, defined by its model and features (color, size, type, etc) & M325 Wireless Optical Scroll Mouse, Brick Red \\ \hline
\end{tabular}
\end{table}

\section{Methods}

Here we will discuss how we concluded on our regression model, selection of variables and interaction of variables, and the evaluation and validation of the model performance. 

\subsection{Model}

The response variable of relevance is \textbf{claimcst0} or the total cost of claims (loss) in a given exposure period for each policy holder. Let us explore regression models to predict this variable.

Let us first consider the use of the most simplest form of regression: the \textbf{Ordinary Least Squares}. Arguably the most commonly used form of regression, the OLS has several assumptions about the data, one of which is that the distribution of the response variable be of a normal distribution. However, from the figure, we see that the distribution of \textbf{claimcst0} is heavilty concentrated around 0, followed by several diminishing frequency bars of minimal size. 
\\

One solution to resolving the violation of the normality assumption would be to consider transofmations of the response, but to better fit the nature of the data, it is ideal to consider the use of \textbf{Generalized Linear Models}, which allows us to select a new distribution that better fits the response variable.
\\

The folks from Travelers introduced us to the \textbf{Tweedie Distribution}, which is a family of probability distributions, namely Poisson and Gamma distributions. The Tweedie distribution allows for the relevant variable to have a positive mass at zero. while following a continuous distribution otherwise. So we considered the \textbf{Tweedie Regression} with a log-link and a variance power of 1.3 (a range of (1,2) corresponds to the Tweedie distribution, otherwise known as the compound Poisson model). 
** Model Outputs?
\\

However, after further research, we decided an alternative approach to modeling claimcst0. We thought of the cost of claims \textbf{claimcst0} as a product of the Frequency of claims (in the given \textbf{exposure} period of the policy) and the Severity of the claim. So we then considered a \textbf{Two-Part Model}, one that would predict the Frequency of claims (\textbf{numclaims}), and another that would predict the average Severity of a claim (\textbf{claimcst0}/\textbf{numclaims}). 
\\

We can think of the cost of claims as a prediction of pure premium as such:

\begin{center}
Pure Premium = $Frequency * Severity$ = $\hat{numclaims} * \hat{\frac{claimcst0}{numclaims}}$
\end{center}

\subsubsection*{Frequency Model}

Let us first discuss the first model: the Frequency Model. We want to model the number of claims of policy holders, which takes on discrete values. Therefore the Frequency Model can be modeled using either the \textbf{Poisson} Regression or the \textbf{Negative Binomial} Regression. If the assumptions of the Poisson distribution are held by the data, it is preferred since it is the natural approach to a count data. Let us check some assumptions.
\\

For the Poisson distribution, the mean and the variance ($\lambda$) should be roughty equal to each other. In our training data set, the mean is -- and the variance is --. This also indicates that there is no sign of overdispersion. Therefore it is appripriate to use the Poisson distribution.
\\

Now let us model this in R. We can use the entire training data for this model since we want to consider all cases, regardless of the occurrence of the claim. For modeling Count, we used log(\textbf{exposure}) as an offset because log(numclaims/exposure) would theoretically give us the estimated count in a fixed period of time (in this case one year), but we are only interested in the numclaims, hence in the model is treated as a response while offset(log(exposure)) is a predictor. We used exposure as an log(offset) instead of as a weight, because when predicting for new values of x, we would not be able to use exposure as a predictor if it is treated as a weight in the model.

\subsubsection*{Severity Model}

Now we will move onto the second model: the Severity Model. This model will take the average claim cost (total claim cost / number of claims) as the response. From the figure----, we see that the distribution of the data has a strong right skewness. Therefore the Severity Model can be modeled using either the \textbf{Gamma} Regression or the \textbf{Inverse Gaussian} Regression. 
\\

For modeling severity, we only want to consider the data for which there was a claim, so the appropriate description of the model would be the mean function of the severity of a claim given that there was a claim. We considered different distributions for this model, but we found that the inverse Gaussian model was the best. Our runner-up was the Gamma regression, but we noticed that the predictions for the Gamma predictions were too dispersed, and gave us some extreme predictions (the right-side tail of the prediction distributions were heavy-tailed). After testing the Inverse Gaussian method (suggested in the book Generalized Linear Models for Insurance Data), we saw that the predictions of claimcst0 for the method had more conservative predictions, so we decided to proceed with Inverse Gaussian GLM.
\\

Ultimately we used the two-part model, and the prediction of \textbf{claimcst0} came from the product of predictions from the Frequency Model and the predictions from the Severity Model. 


\subsection*{Variable Selection}

e.	How did you do you variable selection?  

We performed variable selection first by using the step function (both direction) starting with the full model (considering all variables except veh\_body). After, we evaluated the p-values and use the Anova function from the car package to consider removal of further variables (to prevent overfitting), and evaluating the model on the cross-validation Gini coefficient. We went through a lot of trial and error. 

The reason we did not consider veh\_body as a predictor in either models is because the grouping in veh\_body was too sparse. Some categories contained as little as 9 observations, and we decided that model coefficients computed would not be very accurate, perhaps overfitting on those small sample, which would not be appropriate for this analysis. 

INTERACTIONS


\subsection*{Evaluation and Validation}

d.	How did you evaluate your model (e.g. fit statistics, over-fitting, etc.)?

We evaluated our model fit using cross validation, computing the gini coefficient on the predictions of the training data from the k-fold cross validation models, and after discussion we agreed that the gini coefficient of this would be similar to the gini coefficient of the new predictions for the V or H data. We also used a bootstrapping method to compute an "unbiased" set of coefficients after deciding on a model after model selection.

Goodness of it: deviance, which we did not rely too much on.

Gini coefficient. However, computing the gini coefficient on the training data based on the model produced from the same data does not capture the performance of the model on a new data because it does not consider overfitting. So in order to try to capture the performance of the model on new data, we used Cross Validation.

\subsubsection*{Cross Validation}

According to many sources, we found that a 10-fold cross validation was appripriate. 

For a goodness of fit statistic, we used the gini coefficient based on predictions of the training data from the cross validation models, so that no prediction came from a model that used the observation for which the prediction is made on. 

\subsubsection*{Bootstrapping}

After deciding on the final model, we applied Bootstrapping to compute 




\section*{Results}

Final Model
Gini coefficient from cross-validations
Final model coefficients, model goodness of fit, distribution of predictions, final gini, MSE, etc

\section*{Conclusion}

f.	Any concerns about the resulting model?
g.	What questions to you have about the data?

We had some concern about the data, for example the 0 values in the predictor veh\_value. We were not sure whether these 0's truly meant that the value of a vehicle was 0, or if the 0 meant something else (like NULL or MISSING). We ended up using the 0's in the model, but if we truly understood what those meant we may have changed how we handled those observations. 

Autocorrelation: two individuals may have had been in the same accident, in which case those two would be correlated, but from the information provided we would not know


h.	What variables help explain pure premium (explain to a non-statistician; please include this in your presentation for your business partner)?

I would say vehicle value is one of the most crucial variable in estimating the pure premium, because insuring something of a higher value means that the loss from accidents would be larger, so conversely the premium should be higher. Vehicle age and vehicle body (model) would also be important variables, because intuitively we can say that older vehicles tend to have more frequent problems with vehicle performance that may lead to losses, and probably vehicle performance also varies by body type or model. In terms of the drivers, the age of the driver and possibly gender may be important as well. We can with some confidence say that younger drivers and senior drivers tend to have a higher accident rate than those that fall closer in the middle of the age range. We might also see a difference in accident rate by gender. We could also argue that accident rates differ by area as different neighborhoods could have different levels of safeness of driving, but this might be correlated with the type of drivers that live in the area, in which case gender and age could explain the differences in accident rates by neighborhoods. 

i.	What other variables not in the data set do you think might be useful?

If we had age as a numeric value rather than categories might have a better predictive power (gives us more information). 

Another variable not included in this data set that could have a high potential for estimating pure premium is the driver history or record (driving record or any sort of convictions) which could help estimate how much of a liability the driver could have.


\section*{Appendix}

\subsection*{A: References}

Generalized Linear Models for Insurance Data

\subsection*{B: R Code for Final Predictions} 

\begin{lstlisting}[language=R]
save(group1, file="group1.rda")
\end{lstlisting}


\end{document}