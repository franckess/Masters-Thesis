@Misc{Chang2015,
  Title                    = {shiny: Web Application Framework for R. R package version 0.12.1},

  Author                   = {Chang, W. and Cheng, J. and Allaire, JJ. and Xie, Y. and McPherson, J. },
  Year                     = {2015},

  Type                     = {Computer Program},
  Url                      = {http://CRAN.R-project.org/package=shiny}
}

@Article{RCoreTeam,
  Title                    = {R: A Language and Environment for Statistical Computing},
  Author                   = {{R Core Team}},
  Year                     = {2015},

  Type                     = {Journal Article},
  Url                      = {http://www.R-project.org}
}

@article{Ariel2014,
author = {Ariel, Adelaide and Bakker, Bart F M and de Groot, Mark and van Grootheest, Gerard and van der Laan, Jan and Smit, Jan and Verkerk, Bep},
file = {:C$\backslash$:/Users/rjsai/Dropbox/UMN Courses/Plan B/2014 Record linkage simulation.pdf:pdf},
title = {{Record Linkage in Health Data : a simulation study}},
year = {2014}
}

@article{Christen2012,
abstract = {Record linkage is the process of matching records from several databases that refer to the same entities. When applied on a single database, this process is known as deduplication. Increasingly, matched data are becoming important in many application areas, because they can contain information that is not available otherwise, or that is too costly to acquire. Removing duplicate records in a single database is a crucial step in the data cleaning process, because duplicates can severely influence the outcomes of any subsequent data processing or data mining. With the increasing size of today's databases, the complexity of the matching process becomes one of the major challenges for record linkage and deduplication. In recent years, various indexing techniques have been developed for record linkage and deduplication. They are aimed at reducing the number of record pairs to be compared in the matching process by removing obvious nonmatching pairs, while at the same time maintaining high matching quality. This paper presents a survey of 12 variations of 6 indexing techniques. Their complexity is analyzed, and their performance and scalability is evaluated within an experimental framework using both synthetic and real data sets. No such detailed survey has so far been published.},
author = {Christen, Peter},
doi = {10.1109/TKDE.2011.127},
file = {:C$\backslash$:/Users/rjsai/Dropbox/UMN Courses/Plan B/A{\_}Survey{\_}of{\_}Indexing{\_}Techniques{\_}for{\_}Scal.pdf:pdf},
isbn = {1041-4347},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Blocking,Data linkage,Data matching,Entity resolution,Experimental evaluation,Index techniques,Scalability},
number = {9},
pages = {1537--1555},
title = {{A survey of indexing techniques for scalable record linkage and deduplication}},
volume = {24},
year = {2012}
}

@article{Christen2007,
abstract = {Deduplicating one data set or linking several data sets are increasingly important tasks in the data preparation steps of many data mining projects. The aim of such linkages is to match all records relating to the same entity. Research interest in this area has increased in recent years, with techniques originating from statistics, machine learning, information retrieval, and database research being combined and applied to improve the linkage quality, as well as to increase performance and e?- ciency when linking or deduplicating very large data sets. Di?erent measures have been used to characterise the quality and complexity of data linkage algorithms, and several new metrics have been proposed. An overview of the issues involved in measuring data linkage and deduplication quality and complexity is presented in this chapter. It is shown that measures in the space of record pair comparisons can produce deceptive quality results. Various measures are discussed and recommendations are given on how to assess data linkage and deduplication quality and complexity},
author = {Christen, Peter and Goiser, Karl},
doi = {10.1007/978-3-540-44918-8_6},
file = {:C$\backslash$:/Users/rjsai/Dropbox/UMN Courses/Plan B/qmdm-linkage.pdf:pdf},
isbn = {3540449116},
issn = {1860949X},
journal = {Quality Measures in Data Mining},
keywords = {data integration and matching,data mining pre-processing,data or record linkage,deduplication,quality and complexity measures},
pages = {127--151},
title = {{Quality and complexity measures for data linkage and deduplication}},
url = {http://www.springerlink.com/content/g16460p655166055/},
volume = {151},
year = {2007}
}


@article{Datasets2002,
author = {Datasets, Document and Karypis, George},
file = {:C$\backslash$:/Users/rjsai/Dropbox/UMN Courses/Plan B/d0c3b5314b7bc611c692c309ac36c146df7d.pdf:pdf},
isbn = {1581134924},
keywords = {agglomerative clustering,hierarchical clustering,partitional clus-},
pages = {515--524},
title = {{Evaluation of Hierarchical Clustering Algorithms for}},
year = {2002}
}

@article{Gu2003,
abstract = {Record linkage is the task of quickly and accurately identi- fying records corresponding to the same entity from one or more data sources. Record linkage is also known as data cleaning, entity reconcilia- tion or identification and the merge/purge problem. This paper presents the “standard” probabilistic record linkage model and the associated algorithm. Recent work in information retrieval, federated database sys- tems and data mining have proposed alternatives to key components of the standard algorithm. The impact of these alternatives on the stan- dard approach are assessed. The key question is whether and how these new alternatives are better in terms of time, accuracy and degree of automation for a particular record linkage application.},
author = {Gu, Lifang and Baxter, Rohan},
file = {:C$\backslash$:/Users/rjsai/Dropbox/UMN Courses/Plan B/Gu2003RecordlinkageCurrentpracticeandfuturedirections.pdf:pdf},
journal = {Cmis},
keywords = {data cleaning,entity,entity identification,list washing,merge,object isomerism,purge,reconciliation,record linkage},
pages = {03/83},
title = {{Record linkage: Current practice and future directions}},
url = {http://festivalofdoubt.uq.edu.au/papers/record{\_}linkage.pdf},
year = {2003}
}

@article{Sauleau2005,
abstract = {BACKGROUND: Multiplication of data sources within heterogeneous healthcare information systems always results in redundant information, split among multiple databases. Our objective is to detect exact and approximate duplicates within identity records, in order to attain a better quality of information and to permit cross-linkage among stand-alone and clustered databases. Furthermore, we need to assist human decision making, by computing a value reflecting identity proximity. METHODS: The proposed method is in three steps. The first step is to standardise and to index elementary identity fields, using blocking variables, in order to speed up information analysis. The second is to match similar pair records, relying on a global similarity value taken from the Porter-Jaro-Winkler algorithm. And the third is to create clusters of coherent related records, using graph drawing, agglomerative clustering methods and partitioning methods. RESULTS: The batch analysis of 300,000 "supposedly" distinct identities isolates 240,000 true unique records, 24,000 duplicates (clusters composed of 2 records) and 3,000 clusters whose size is greater than or equal to 3 records. CONCLUSION: Duplicate-free databases, used in conjunction with relevant indexes and similarity values, allow immediate (i.e. real-time) proximity detection when inserting a new identity.},
author = {Sauleau, Erik a and Paumier, Jean-Philippe and Buemi, Antoine},
doi = {10.1186/1472-6947-5-32},
file = {:C$\backslash$:/Users/rjsai/Downloads/1472-6947-5-32.pdf:pdf},
issn = {1472-6947},
journal = {BMC medical informatics and decision making},
pages = {32},
pmid = {16219102},
title = {{Medical record linkage in health information systems by approximate string matching and clustering.}},
volume = {5},
year = {2005}
}

@article{Uw2001,
abstract = {Despite many empirical successes of spectral clustering methods| algorithms that cluster points using eigenvectors of matrices derived from the data|there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly dierent ways. Second, many of these algorithms have no proof that they will actually compute a reasonable clustering. In this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results on a number of challenging clustering problems. 1},
author = {Uw, S and Ng, Andrew Y and Jordan, Michael I and Weiss, Yair},
doi = {10.1.1.19.8100},
file = {:C$\backslash$:/Users/rjsai/Dropbox/UMN Courses/Plan B/nips01-spectral.pdf:pdf},
isbn = {1049-5258},
issn = {{\textless}null{\textgreater}},
journal = {Advances in Neural Information Processing Systems 14},
keywords = {clustering community detection graph spectral theo},
pages = {849--856},
title = {{On spectral clustering: Analysis and an algorithm}},
year = {2001}
}

@article{VanderLoo2014,
abstract = {Comparing text strings in terms of distance functions is a common and fundamental task in many statistical text-processing applications. Thus far, string distance functionality has been somewhat scattered around R and its extension packages, leaving users with inconistent interfaces and encoding handling. The stringdist package was designed to offer a low-level interface to several popular string distance algorithms which have been re-implemented in C for this purpose. The package offers distances based on counting q-grams, edit-based distances, and some lesser known heuristic distance functions. Based on this functionality, the package also offers inexact matching equivalents of R's native exact matching functions match and {\%}in{\%}.},
author = {van der Loo, Mark},
file = {:C$\backslash$:/Users/rjsai/Dropbox/UMN Courses/Plan B/loo.pdf:pdf},
issn = {20734859},
journal = {The R Journal},
number = {1},
pages = {111--122},
title = {{{\{}stringdist{\}}: an {\{}R{\}} Package for Approximate String Matching}},
volume = {6},
year = {2014}
}
